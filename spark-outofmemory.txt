https://courses.trendytech.in/



2 Executors:
   with 512 mb
   with 4 cores
   
   means that it will tigger two executors with 512 mb memory for each and each executor will trigger 4 threads to process data.
   each executor has jvm process. 
   jvm has on heap and off heap memory.
   On heap is default memory managed by jvm and off heap memory is managed by OS and bydefalut it is disabled.
   
   so now we have 512 mb for executors jvm will not allow to take entire memory, it will allow only 89% of memory remaining memory will 
   hold for Garabage Collector and other operations.
   
   usable memory is only 89% i.e 455.69 mb for executor.
   out of this spark can not use all memory for process:
    SparkResevedMemory is 300 mb for to store spark internals. remaining will be used for spark processing.
    Minimum amount of memory you request for executors is 1.5 times for reseved memory i.e 1.5* 300 mb =450Mb 
    That means that we cannot create spark executor with less than 450 mb it should be more than 450 mb.
   
   Now we have 155.69 mb for spark process out of 455.69 mb 
   
   Again this 155.69 mb divides into two :
    unified spark memory (60%):spark has control on this memory. i.e 93.4mb
	user memory(40%): is used to store datastructres,variables), spark doesn't have control on this memory. i.e 62.28 mb
	we can increase decrease this memory by setting this property spark.memory.fraction=0.6 by default 60%
	
 Again in unified spark memory divides two parts(93.4mb): 
    one is storage memory : used to cache your data in spark applications.
	one is Execution : it stores hash tables, aggregations data,shuffle data, and it has high priority.
	spark.memory.storageFraction=0.5 by default each has half of the memory i.e now 93.4/2 =46.7mb for each 
	but both can use others memory when it's free.
	
	1)storage memory can barrow some from execution memory only if execution memory is free.
	2)Execution memory can barrow from storage memory if it's empty and has not reached it's storage fraction limit.if needed it can evict.
	3)Execution memory used by storage memory Execution needs more memory ,it can forcefully evict the memory occupied by  storage memory at 
	execution side.
	4)Strorage needs more memory ,it cannot forcefully evict the excess blocks occupied by execution memory , it has to wait for execution.
	
	
STORAGE SPILL:	
when storage needs more memory but it's not aviable then it will spill data into disk.
if your persist option is MEMORY_AND_DISK,if it's is MEMORY_ONLY then it has to recycle the memory we may get OOM errors from storage side.

if it's spill to disk , it will store data into serialization ,when it has to read it will deserialize and it is a costly operation.


Exeution memory will again split into parts based on number of cores.
here we have configured 4 cores i.e 46.7mb/4 =11.675 for each thread.

EXECUTION SPILL:
For example we have 40mb of ram for executions we have 2 cores
we are reading two partitions with one partitions size 25mb and othere 5 mb.
one core will read 25 mb one core  will read 5 mb. this is we called as data skew ness.
As each core have 20 mb one thread cannot read 25 mb so it will spill to read 25 mb partitions into memory.
spill will happen in desiraliztion(convert java object to bytestreams). 


OOM-1
For example spark is reading one partitions in which each record size is more than execution memory then it leads to OOM.
because it's cannot split the record.
it can spill the data but it cannot spilt the records.

OOM-2
if shuffle data not fit into memory then also we will get OOM.

OOM-3 
if you are  broadcast variable and if it's not fit into in memory it will leads to OOM

OOM-4
For example in case of data explotion 
i.e  while doing cross join or explode operation
where 10 mb data mulitply and can leads to OOM 



OFF HEAP MEMORY:
to enable this we have to use below two properties.
spark.memory.offheap.enabled=true
spark.memory.offheap.size=512mb

off heap can use by storage memory to store cached data.
limitations:
we cannot use off heap memory because JVM GC will not happen here you have to implement code to do Garbage Collection.
GC is not maintenained by JVM. 
JVM only managed only ON Heap Memory.

what is GarabageCollections:

JVM is responsibel to run GB. 
GB is responsible to remove unused objects from memory.

spark can also fails because of GC takes logger times to clean.
or creation of objects is more and cleaning of gc is less then leads to OOM.


we can print GC logs in spark by configuring spark.driver.extraJavaOptions, spark.executor.extraJavaOPtions 
we can use GC logs to check how GC functions in the application.

we can use gclogs to analysis of you GC by using Geasy tool.
it will give you how you GC working in your applications.


and if you are getting out of memory because of memory leaks and you don't have idea where you have to look.
then you can write heapdump.bin log using jvm options when heapoutofmemory occure then we can analysis that using esay tool.
------------------------------------------------------------------------








